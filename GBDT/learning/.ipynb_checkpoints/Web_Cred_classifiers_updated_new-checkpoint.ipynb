{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/ab/43e678759326f728de861edbef34b8e2ad1b1490505f20e0d1f0716c3bf4/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl (20.0MB)\n",
      "\u001b[K     |██████████████████▊             | 11.7MB 120kB/s eta 0:01:09"
     ]
    }
   ],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e4f2388829c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.metrics import f1_score as f1\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import cohen_kappa_score as kappa_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2, SelectPercentile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "def xfrange(start, stop, step):\n",
    "    i = 0\n",
    "    while start + i * step < stop:\n",
    "        yield start + i * step\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def getclasscount(Y_train):\n",
    "    classes = list(set(Y_train))\n",
    "    print (classes)\n",
    "#     class_name = ['article', 'help', 'shop', 'public_portrayals_companies_and_institutions', 'discussion', 'link_collection', 'downloads', 'private_portrayal_personal_homepage', 'other']\n",
    "    class_name = ['public_portrayals_companies_and_institutions', 'article', 'link_collection', 'discussion', 'shop']\n",
    "    class_counts = [0]*len(class_name)\n",
    "    for i in Y_train:\n",
    "        class_counts[int(i)] += 1\n",
    "    return [(class_name[int(i)], class_counts[int(i)]) for i in classes]    \n",
    "\n",
    "def Getclasscount(Y_train):\n",
    "    classes = list(set(Y_train))\n",
    "    print (classes)\n",
    "    class_counts = {}\n",
    "    for i in Y_train:\n",
    "        if not class_counts.get(i):\n",
    "            class_counts[i] = 0\n",
    "        class_counts[i] += 1\n",
    "    return class_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def stats(y_test, y_pred):\n",
    "    \n",
    "    report1 = kappa_score(y_test, y_pred)   \n",
    "\n",
    "    report2 = classification_report(y_test, y_pred)\n",
    "    print ('\\n clasification report:\\n', report2)\n",
    "    print ('\\n confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n",
    "    return report1,report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# biased learning, most of thr predictions were articles, \n",
    "# as it has the maximum count in training set\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(class_weight='balanced',C=1e10, gamma='auto')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# nodes = [512,256,32]\n",
    "nodes = [128,32]\n",
    "print ('nodes=', nodes)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=nodes)\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=nodes,learning_rate='adaptive',batch_size=1000,max_iter=100,verbose=True,early_stopping=True)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt = GradientBoostingClassifier(n_estimators=100,max_depth=3,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "X = []\n",
    "\n",
    "\n",
    "# read data\n",
    "# with open('webcred_features_expanded.csv','rb') as f:\n",
    "with open('../data/dump/health/webcred_features_expanded.csv','rb') as f:\n",
    "    data = csv.reader(f, delimiter=',')\n",
    "    for line in data:\n",
    "        X.append(line)\n",
    "\n",
    "        \n",
    "tag = X[0].index('genre')\n",
    "\n",
    "# Y = [i[tag] for i in X]\n",
    "# Y = Y[1:]\n",
    "# X = [row[:tag]+row[tag+1:] for row in X]\n",
    "\n",
    "\n",
    "features = X[0]\n",
    "del features[tag]\n",
    "\n",
    "print ('Features Count=' ,len(features))\n",
    "\n",
    "X = X[1:]\n",
    "        \n",
    "\n",
    "X_2 = []\n",
    "Y = []\n",
    "article_count = 0\n",
    "portrayal_count = 0\n",
    "link_collection = 0\n",
    "\n",
    "# class_name = ['article', 'help', 'shop', 'public_portrayals_companies_and_institutions', 'discussion', 'link_collection', 'downloads', 'private_portrayal_personal_homepage', 'other']\n",
    "class_name = ['public_portrayals_companies_and_institutions', 'article', 'link_collection', 'discussion', 'shop']\n",
    "count = {i:0 for i in class_name}\n",
    "\n",
    "class_count_threshold = 20000\n",
    "\n",
    "for index, row in enumerate(X):\n",
    "\n",
    "    genre = row[tag]\n",
    "\n",
    "    # removing broken_links and other\n",
    "    if genre in [\n",
    "#                 '8',\n",
    "                 '4',\n",
    "#                     '6',\n",
    "#                     '1'\n",
    "                ]:\n",
    "        continue\n",
    "    \n",
    "    temp = []    \n",
    "    for index, item in enumerate(row):\n",
    "        if index==tag:\n",
    "            \n",
    "            try:\n",
    "                count[class_name[int(item)]]+=1\n",
    "#                 count[item]+=1\n",
    "#                 print item\n",
    "            except:\n",
    "                print( item)\n",
    "                break\n",
    "\n",
    "            if count[class_name[int(item)]]>class_count_threshold:\n",
    "#             if count[item]>class_count_threshold:\n",
    "                    temp = []\n",
    "                    break\n",
    "#             if item=='0':\n",
    "#                 article_count += 1\n",
    "#                 if article_count>100:\n",
    "#                     temp = []\n",
    "#                     break\n",
    "#             elif item=='3':\n",
    "#                 portrayal_count += 1\n",
    "#                 if portrayal_count>100:\n",
    "#                     temp = []\n",
    "#                     break\n",
    "#             elif item=='5':\n",
    "#                 link_collection += 1\n",
    "#                 if link_collection>100:\n",
    "#                     temp = []\n",
    "#                     break\n",
    "\n",
    "            Y.append(item)    \n",
    "        else:    \n",
    "            # fill empty cells with 0 \n",
    "            if item is not '':\n",
    "                item = float(item)\n",
    "            else:\n",
    "                item = 0.0\n",
    "            temp.append(item)\n",
    "    if temp:        \n",
    "        X_2.append(temp)\n",
    "    \n",
    "X = X_2\n",
    "\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "Y = np.ravel(Y)\n",
    "\n",
    "print('Sample size=' ,len(Y))\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# print 'X_train >>' + str(X_train.shape) + ' Training data dimension'\n",
    "# print 'Y_train >>' + str(Y_train.shape) + ' Training data labels'\n",
    "\n",
    "# print 'X_test >>' + str(X_test.shape) + ' Test data dimension'\n",
    "# print 'Y_test >>' + str(Y_test.shape) + ' Test data labels'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print ('Training counts')\n",
    "getclasscount(Y)\n",
    "# Getclasscount(Y)\n",
    "# print \n",
    "# print 'Testing counts'\n",
    "# print getclasscount(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove features which have same value across all samples\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# threshold>> Defaults to 0.0, e.g. only remove features with the same value in all samples\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "X_var = sel.fit_transform(X,Y)\n",
    "# X_test_var = sel.transform(X_test)\n",
    "\n",
    "print ('After variance thresholding')\n",
    "print (str(X_var.shape) + ' Training data dimension')\n",
    "# print str(Y_train.shape) + ' Training data labels'\n",
    "\n",
    "# print str(X_test_var.shape) + ' Test data dimension'\n",
    "# print str(Y_test.shape) + ' Test data labels'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# name of selected features based on Variance\n",
    "\n",
    "selected_features = sel.get_support(indices=True)\n",
    "print (selected_features)\n",
    "# print [features[feats] for feats in selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ANNOVA scores for each feature\n",
    "\n",
    "# skb = SelectPercentile(f_classif,percentile=20)\n",
    "# X_skb = skb.fit_transform(X_var,Y_train)\n",
    "# X_test_skb = skb.transform(X_test_var)\n",
    "annova,pval = f_classif(X_var,Y)\n",
    "print (annova.shape)\n",
    "# print X_test_skb.shape\n",
    "# print skb.scores_\n",
    "# print skb.get_support(indices=True)\n",
    "# print annova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(annova)\n",
    "plt.grid(True)\n",
    "plt.ylabel('Annova Score')\n",
    "plt.xlabel(\"Feature's Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Generate Mutual Information gain\n",
    "\n",
    "# mkb = SelectKBest(mutual_info_classif,k=200)\n",
    "# X_mkb = mkb.fit_transform(X_train,Y_train)\n",
    "# X_test_mkb = mkb.transform(X_test)\n",
    "mig = mutual_info_classif(X_var,Y,n_neighbors=5)\n",
    "print (mig.shape)\n",
    "# print X_test_mkb.shape\n",
    "# print skb.scores_\n",
    "# print mkb.get_support(indices=True)\n",
    "\n",
    "# features[selected_features[np.argmax(mig)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(mig)\n",
    "plt.grid(True)\n",
    "plt.ylabel('MIG Score')\n",
    "plt.xlabel(\"Feature's Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select features with ANNOVA scores above 5\n",
    "def build_skb(score):\n",
    "    X_skb = X_var[:,annova>=score]\n",
    "#     X_test_skb = X_test_var[:,annova>=score]\n",
    "    \n",
    "    \n",
    "    print ('Annova Score ' + str(score))\n",
    "    print ('Feature Count ' + str(X_skb.shape[1]))\n",
    "    \n",
    "#     feats = [elem for index, elem in enumerate(selected_features) if annova[index]>=score]\n",
    "#     print [features[i] for i in feats]\n",
    "#     print\n",
    "\n",
    "    return X_skb\n",
    "\n",
    "# Select features with Mutual Information gain above 0.05\n",
    "def build_mkb(score):\n",
    "    X_mkb = X_var[:,mig>=score]\n",
    "#     X_test_mkb = X_test_var[:,mig>=score]\n",
    "\n",
    "    print( 'MIG Score ' + str(score))\n",
    "    print ('Feature Count ' + str(X_mkb.shape[1]))\n",
    "\n",
    "#     feats = [elem for index, elem in enumerate(selected_features) if mig[index]>=score]\n",
    "#     print [features[i] for i in feats]\n",
    "#     print\n",
    "    \n",
    "    return X_mkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# under sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids\n",
    "\n",
    "# cluster sample\n",
    "ccsampling = ClusterCentroids(random_state=45,sampling_strategy='all')\n",
    "\n",
    "# random sample \n",
    "russampling = RandomUnderSampler(random_state=0)\n",
    "\n",
    "# print 'feature selection- none'\n",
    "# print \"initial dimension of matrix\"\n",
    "# print str(X_train.shape) + ' Training data dimension after feature selection'\n",
    "# print str(Y_train.shape) + ' Training labels'\n",
    "\n",
    "# print 'After random under sampled'\n",
    "# X_train_rus, Y_train_rus = russampling.fit_sample(X_train, Y_train)\n",
    "# print str(X_train_rus.shape) + ' Training data dimension'\n",
    "# print str(Y_train_rus.shape) + ' Training labels'\n",
    "\n",
    "# getclasscount(Y_train_rus)\n",
    "\n",
    "# print 'After cluster sampling'\n",
    "# X_train_cc, Y_train_cc = ccsampling.fit_sample(X_train, Y_train)\n",
    "# print str(X_train_cc.shape) + ' Training data dimension'\n",
    "# print str(Y_train_cc.shape) + ' Training labels'\n",
    "\n",
    "# getclasscount(Y_train_cc)\n",
    "\n",
    "# more likely ccsampling is better over russampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# over sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smsampling = SMOTE(random_state=2,k_neighbors=1)\n",
    "\n",
    "# print \"initial dimension of matrix\"\n",
    "# print str(X_mkb.shape) + ' Training data dimension after feature selection'\n",
    "# print str(Y_train.shape) + ' Training labels'\n",
    "\n",
    "# X_mkb_ros, Y_train_ros = smsampling.fit_sample(X_mkb, Y_train)\n",
    "\n",
    "# print 'After Oversampling'\n",
    "# print str(X_mkb_ros.shape) + ' Training data dimension'\n",
    "# print str(Y_train_ros.shape) + ' Training labels'\n",
    "\n",
    "# getclasscount(Y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "features_reduced_data = {\n",
    "#     'unreduced':X \n",
    "#     'variance':X_var, \n",
    "    'Annova':'build_skb', \n",
    "#     'Mutual Info':'build_mkb'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sampling_clusters = {\n",
    "#     'cluster sampling': ccsampling, \n",
    "    'over sampling': smsampling\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_models = {\n",
    "#     'svm':svm,\n",
    "#     'LogisticRegression':lr, \n",
    "#     'neural networks':mlp,\n",
    "    'GradientBoostingClassifier':gbdt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# CLassifier Function\n",
    "def train_clf(x,y, clf, model_name):\n",
    "        \n",
    "    for sample_name, sample_func in sampling_clusters.items():\n",
    "        \n",
    "\n",
    "#         print sample_name\n",
    "        kf.get_n_splits(x)\n",
    "        training = 0\n",
    "        testing = 0\n",
    "        x, y = sample_func.fit_sample(x,y)\n",
    "        \n",
    "        print (getclasscount(y))\n",
    "        return\n",
    "                        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "        clf.fit(x_train,y_train)    \n",
    "        train = clf.score(x_train, y_train)\n",
    "        test = clf.score(x_test, y_test)\n",
    "        \n",
    "        print ('Training Accuracy ' + str(train))\n",
    "        print ('Testing Accuracy ' + str(test*100))\n",
    "\n",
    "        if test>=0.8:\n",
    "            y_pred = clf.predict(x_test)\n",
    "\n",
    "            kappa_score, classification_report = stats(y_test, y_pred)\n",
    "        \n",
    "        return test\n",
    "        \n",
    "        for train_index, test_index in kf.split(x):\n",
    "\n",
    "            x_train, x_test = x[train_index], x[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            clf.fit(x_train,y_train)\n",
    "\n",
    "            train = clf.score(x_train, y_train)\n",
    "            test = clf.score(x_test, y_test)\n",
    "            \n",
    "            training += train\n",
    "            testing += test\n",
    "            \n",
    "            print ('Training Accuracy ' + str(train))\n",
    "            print ('Testing Accuracy ' + str(test*100))\n",
    "\n",
    "\n",
    "            \n",
    "            if test>=0.8:\n",
    "                y_pred = clf.predict(x_test)\n",
    "\n",
    "                kappa_score, classification_report = stats(y_test, y_pred)\n",
    "    #             getclasscount(y_test)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "            print ('--------------------------------------------------------------')\n",
    "        return testing/folds*100.0\n",
    "        if testing/10>=0.8:\n",
    "#             print sample_name\n",
    "#             print 'Training Accuracy ' + str(training/10)\n",
    "#             print 'Testing Accuracy ' + str(testing/10)\n",
    "            print (str(training/10))\n",
    "            print (str(testing/10))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#     with open('training.pkl','a') as f:\n",
    "#         pickle.dump(kappa_score, f)\n",
    "#         pickle.dump(classification_report, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_graph = []\n",
    "\n",
    "for model_name, model_func in training_models.items():\n",
    "\n",
    "    print( model_name)\n",
    "\n",
    "    for name,data in features_reduced_data.items():\n",
    "\n",
    "        print (name)\n",
    "        testing = []\n",
    "        feature_count = []\n",
    "\n",
    "        if isinstance(data,str):\n",
    "            # perform k-fold training and testing modelling\n",
    "            for i in xfrange(7.5,7.6,0.5):\n",
    "#             for i in xfrange(0.02,0.16,0.02):\n",
    "#                 print i\n",
    "                x = eval(data)(i)\n",
    "\n",
    "                testing.append(train_clf(x, Y, model_func, model_name))\n",
    "                feature_count.append(x.shape[1])\n",
    "\n",
    "                # train_clf(x_train, y_train, x_test, y_test, model_func, model_name)\n",
    "#                 print\n",
    "\n",
    "\n",
    "        data_graph.append(go.Scatter(\n",
    "            x = feature_count,\n",
    "            y = testing,\n",
    "            mode = 'lines',\n",
    "            name = model_name\n",
    "        ))                \n",
    "   \n",
    "    print( '--------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Testing Accuracy across Models for various {0} Filtering'.format(name),\n",
    "              xaxis = dict(title = 'Feature Count'),\n",
    "              yaxis = dict(title = 'Testing Accuracy (%)'),\n",
    "              )\n",
    "\n",
    "fig = dict(data=data_graph, layout=layout)    \n",
    "\n",
    "py.iplot(fig, filename='line-mode')\n",
    "\n",
    "# done\n",
    "#get report of 4k pages\n",
    "#repeat the exp. with individual csv of serc labels\n",
    "#. Mohit 150, \n",
    "\n",
    "# done\n",
    "#calculate gcs for new pages >> in progress\n",
    "#calculate similarity score >>\n",
    "#     input security group URLs\n",
    "#        WIKI-SEED URL, C3 in progress"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
